{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Blackjack-v0\")\n",
    "\n",
    "# The typical imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pi(action, state, action_space, q):\n",
    "    \"\"\"pi(a,s,A,V) := pi(a|s)\n",
    "    We take the argmax_a of Q(s,a).\n",
    "    q[s] = [q(s,0), q(s,1), ...]\n",
    "    \"\"\"\n",
    "    argmax = max(action_space, \n",
    "                 key=(lambda key: q.get(state,[0]*len(action_space))[key]))\n",
    "    return argmax == action # 1 if it's greedy, 0 if not.\n",
    "\n",
    "\n",
    "def b(action, state, action_space, q, epsilon=0.6):\n",
    "    \"\"\"b(a,s,A) := b(a|s) \n",
    "    Sometimes you can only use a subset of the action space\n",
    "    given the state.\n",
    "    \n",
    "    Randomly selects an action from a uniform distribution.\n",
    "    \"\"\"\n",
    "    return epsilon/len(action_space) + (1-epsilon) * pi(action, state, action_space, q)\n",
    "\n",
    "\n",
    "def generate_returns(ep, gamma=1, action_value=False):\n",
    "    G = {} # return on state\n",
    "    C = 0 # cumulative reward\n",
    "    for tpl in reversed(ep):\n",
    "        observation, action, reward = tpl\n",
    "        if action_value:\n",
    "            G[(observation, action)] = C = reward + gamma*C\n",
    "        else:\n",
    "            G[observation] = C = reward + gamma*C\n",
    "    return G\n",
    "\n",
    "\n",
    "def get_importance_ratio(ep, pi, b, action_space, q):\n",
    "    prev_ratio = 1\n",
    "    ratios = []\n",
    "    for tpl in reversed(ep):\n",
    "        observation, action, _ = tpl\n",
    "        cur_ratio = prev_ratio * \\\n",
    "            pi(action, observation, action_space, q)/b(action, observation, action_space, q)\n",
    "        ratios.append(max(cur_ratio, 1e-7))\n",
    "        prev_ratio = cur_ratio\n",
    "    return reversed(ratios)\n",
    "\n",
    "def choose_action(policy, state, ACTION_SPACE, Q):\n",
    "    probs = [policy(a, state, ACTION_SPACE, Q) for a in ACTION_SPACE]\n",
    "    return np.random.choice(ACTION_SIZE, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2934\n",
      "-0.309\n",
      "-0.273\n",
      "-0.2571\n",
      "-0.2759\n"
     ]
    }
   ],
   "source": [
    "def result(env, pi, ACTION_SPACE, Q):\n",
    "    rewards = []\n",
    "    for _ in range(10000):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            action = choose_action(pi, observation, ACTION_SPACE, Q)\n",
    "            # action = env.action_space.sample()\n",
    "            #print(\"Started out with: {}\".format(observation))\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            #print(\"Picked: {}, got obs: {}\".format(action, observation))\n",
    "            if done:\n",
    "                rewards.append(reward)\n",
    "                #print(\"Ended with: {}\".format(reward))\n",
    "                break\n",
    "    return np.mean(rewards)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Run a demo of the environment\n",
    "for eps in [100, 1000,10000,100000,1000000]:\n",
    "    Q = {}\n",
    "    ACTION_SIZE=2\n",
    "    ACTION_SPACE=(0, 1)\n",
    "    C = {} # cumulative rho\n",
    "\n",
    "    for i in range(eps):\n",
    "        ep = []\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            # Choosing behavior policy\n",
    "            action = choose_action(b, observation, ACTION_SPACE, Q)\n",
    "\n",
    "            # Run simulation\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            ep.append((observation, action, reward))\n",
    "\n",
    "            observation = next_observation\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Generate returns, return ratio\n",
    "        G = generate_returns(ep, gamma=0.8, action_value=True)\n",
    "        rhos = get_importance_ratio(ep, pi, b, ACTION_SPACE, Q)\n",
    "        # 1 iteration of value iteration\n",
    "        for s, rho in zip(G, rhos):\n",
    "            state, action = s\n",
    "\n",
    "            # Getting q's action\n",
    "            Q[state] = Q.get(state, [0]*ACTION_SIZE)\n",
    "            q = Q[state][action]\n",
    "\n",
    "            # Getting cumulative rho\n",
    "            C[state] = C.get(state, [0]*ACTION_SIZE)\n",
    "            prev_c = C[state][action]\n",
    "            C[state][action] += rho\n",
    "\n",
    "            Q[state][action] = q * prev_c/C[state][action] + G[s] * rho /C[state][action]\n",
    "            \n",
    "    res = result(env, b, ACTION_SPACE, Q)\n",
    "    print(res)\n",
    "    results.append(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(Q)\n",
    "pprint(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q = {}\n",
    "# ACTION_SIZE=2\n",
    "# ACTION_SPACE=(0, 1)\n",
    "# C = {} # cumulative rho\n",
    "# for ep in episodes[:2]:\n",
    "#     G = generate_returns(ep, gamma=0.5, action_value=True)\n",
    "#     rhos = get_importance_ratio(ep, pi, b, ACTION_SPACE, Q)\n",
    "#     for s, rho in zip(G, rhos):\n",
    "#         observation, action = s\n",
    "        \n",
    "#         # Getting q's action\n",
    "#         actions = Q.get(observation, [0]*ACTION_SIZE)\n",
    "#         q = actions[action]\n",
    "        \n",
    "#         # Getting cumulative rho\n",
    "#         cum_rhos = C.get(observation, [1e-5]*ACTION_SIZE)\n",
    "#         cum_rhos[action] += rho\n",
    "        \n",
    "#         actions[action] = q + rho/cum_rhos[action]*(G[s] - q)\n",
    "#         Q[observation] = actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a 3D wireframe like in the example mplot3d/wire3d_demo\n",
    "X = np.arange(4, 21)\n",
    "Y = np.arange(1, 10)\n",
    "Z = np.array([np.array([Q[(x, y, False)][0] for x in X]) for y in Y])\n",
    "X, Y = np.meshgrid(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y.shape, Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        action = choose_action(pi, observation, ACTION_SPACE, Q)\n",
    "        # action = env.action_space.sample()\n",
    "        #print(\"Started out with: {}\".format(observation))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #print(\"Picked: {}, got obs: {}\".format(action, observation))\n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            #print(\"Ended with: {}\".format(reward))\n",
    "            break\n",
    "print(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
